Foundational principles are the basic roots and may sound like cliche. But I like cliches because they are true. And stronger the roots, stronger the tree!

Mentioning some basic foundational aspects of Software Testing.

Program: is a set of instructions to a computer.
Software: is a collection of programs that help us to perform a task.
Types of software:
*System software: device drivers, operating systems, utilities and servers.
*Programming software: compilers, debuggers and interpreters.
*Application software: web apps, mobile apps and desktop apps.

Software testing: is a part of Software Development Life Cycle(SDLC). It is an activity to detect and identify the defects in the software, so that we could deliver the quality product to the customer/client.

Well what is software quality: customer justification, whether the product is working according to their expectations or not.
What are its parameters: it must be,
*Bug-free.
*Delivered on time.
*Within budget.
*Meets customer expectations.
*Maintainable(user-friendly in their environment).

Difference betweeen a Project and a Product: if the software is developed for a specific client based on their requirements then it is called a project. Whereas if the software is developed for multiple customers based on market requirements then it is called as a product.

Error: is a human mistake while writing the program.
Bug: while testing if any action misbehaves other than the expected action(for example: logs in even after providing incorrect credentials).
Failure: the task which end user identifies that it is not according to their expectations.

Why does the software has bugs:
*Programming errors.
*Software complexities.
*Changing requirements.
*Lack of skilled testers.
*Miscommunication or no communication between the documentation, development and testing phases.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Continuation of foundational aspects of Software Testing.

SDLC: Software Development Life Cycle is a process followed by software industries to design, develop and test the softwares. It's phases are,
*Requirement Analysis.
*Design.
*Development.
*Testing.
*Deployment & Maintenance.

Models in SDLC:
*Waterfall model: is a sequential software development process model which follows a linear and structured approach. It involves,
Requirements gathering: collecting the requirements from customer and preparing the Software requirement specification(SRS) document.
System design: based on the requirements document, the design document is prepared.
Development: based on the design document, implementation starts.
Testing: after implementation, testing the software.
Deployment & maintenance: deploy the software in customer's environment and customer starts using the software.

Advantages:
*Quality of the product will be good.
*Since requirement changes are not allowed, chances of bugs are less.
*Initial investement is low since the testers are hired in later stages.
Disadvantages:
*Requirement changes are not allowed.
*If there is a defect in requirements phase then that will be continued in later stages.
*Total investment is high because rework on a defect is time consuming.

*Spiral model: the process is followed in more than three cycles to release different versions of softwares. It involves,
Requirement analysis, prototype, development & testing, customer evaluation, planning, risk analysis, engineering & execution, evaluation.
It overcomes the drawbacks of waterfall model. Whenever there is a dependency on modules, we follow this model.

Advantages:
*Testing is done after every cycle before going to the next cycle.
*Customer gets to use the software for every module.
*Requirement changes are allowed after every cycle before going to the next cycle.
Disadvantages:
*Requirement changes are not allowed within the cycle.
*Every cycle of spiral model looks like waterfall model.

Prototype model: nothing but a blueprint of software.
*Requirements are gathered from customer.
*A prototype is developed.
*Demonstrating it to the customer.
*If the customer is satisfied, then we design, develop and test the software.
This model comes between waterfall and spiral models.

V model: verification and validation model.
Verification: 
*Checks whether we are building the right product(not yet ready) or not.
*Focuses on documentation.
*Typically involves static testing techniques(testing the project related documents like BRS, SRS, HLD & LLD through processes like review, walkthrough and inspection).
Validation:
*Checks whether we are building the product(ready) right or not.
*Focuses on software.
*Involves dynamic testing techniques( testing the actual software through unit, integration, system and UAT testings).

Understanding the Software Development Life Cycle (SDLC) is crucial for QA testers for several reasons:
-------------------------------------------------------------------------------------------------------
1. Comprehensive Knowledge of the Development Process
Contextual Understanding: Knowing the SDLC gives QA testers a broad perspective of the entire software development process. It helps them understand how their work fits into the bigger picture.
Interconnected Phases: Each phase of the SDLC impacts the next. Understanding this interconnectedness helps testers anticipate what they need to prepare for and what to expect at each stage.
2. Effective Communication and Collaboration
Stakeholder Interaction: QA testers often interact with various stakeholders, including developers, project managers, and business analysts. A solid grasp of the SDLC enables effective communication and collaboration.
Shared Vocabulary: Familiarity with SDLC terms and concepts ensures that everyone is on the same page, reducing misunderstandings and enhancing teamwork.
3. Early Involvement and Better Planning
Shift-Left Testing: Understanding SDLC encourages early involvement of QA in the development process. Early testing helps identify defects at the initial stages, reducing the cost and time required for fixes.
Test Planning: Knowledge of the different SDLC phases helps QA testers in planning their testing activities effectively, aligning them with the project timelines and milestones.
4. Quality Assurance Throughout the Lifecycle
Continuous Quality Focus: QA is not just about testing the final product but ensuring quality throughout the development process. Understanding SDLC ensures that QA testers contribute to every phase, from requirements to maintenance.
Defect Prevention: By understanding each phase, QA testers can help in preventing defects by ensuring requirements are clear, designs are reviewed, and code is checked for standards and best practices.
5. Efficient Test Case Design
Requirement Analysis: Knowledge of the SDLC helps QA testers in understanding and analyzing requirements thoroughly, which is crucial for designing effective test cases.
Design Insights: Understanding the design phase helps testers in identifying critical areas to focus on during testing, ensuring comprehensive test coverage.
6. Effective Use of Testing Tools and Techniques
Tool Integration: Different phases of the SDLC may require different tools and techniques (e.g., unit testing tools during implementation, performance testing tools during testing). Understanding the SDLC helps QA testers choose and integrate the right tools at the right time.
Technique Selection: QA testers can apply appropriate testing techniques (e.g., black-box testing during system testing, white-box testing during integration testing) based on their understanding of the SDLC phases.
7. Enhanced Problem-Solving Skills
Root Cause Analysis: Understanding the development lifecycle enables QA testers to perform effective root cause analysis of defects, tracing issues back to their source in earlier phases.
Proactive Issue Resolution: With a good grasp of the SDLC, QA testers can proactively suggest improvements and preventive measures in the development process, enhancing overall software quality.
8. Career Advancement and Professional Growth
Skill Enhancement: A deep understanding of SDLC makes QA testers more versatile and valuable team members, as they can contribute beyond traditional testing roles.
Leadership Opportunities: Knowledge of SDLC is essential for QA professionals aspiring to move into leadership roles, such as QA Manager or Test Lead, where strategic planning and process improvement are key responsibilities.
Conclusion
Understanding the SDLC is fundamental for QA testers as it equips them with the knowledge to ensure quality at every stage of the software development process. It enhances their ability to plan, communicate, and execute testing activities effectively, ultimately contributing to the delivery of high-quality software products.

White-box testing technique: testing the code through unit and integration testings.
Black-box testing technique: testing the functionality and flow through system and UAT testings.
Grey-box testing technique: combination of white-box and black-box testing techniques(example: database testing).

Quality Assurance vs Quality Control:
*QA is process related, QC is actual testing of software.
*QA is building in quality, QC is testing for quality.
*QA is preventing the defects, QC is detecting the defects.
*QA is process oriented, QC is product oriented.
*QA is entire lifecycle, QC is testing part of software lifecycle.

Quality Engineering: is writing the automation scripts for testing the software.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Levels of testing:
Unit testing: conducting testing on a single unit/component. A unit or a component is a single module of the software. Comes under white-box testing technique and is conducted by developers. It covers,
*Basis path testing(whether every path is covered or not).
*Control structure testing:
-Conditional coverage(conditions).
-Loop coverage(loops).
-Mutation testing(testing with multiple sets of data).

Integration testing: checking the data communication between two or more modules. Comes under white-box testing technique. Internal coding is conducted by developers and external UI interface is tested by testers. Data flow between multiple modules is tested. It involves:
*Incremental integration testing:
-Top-down approach: Incrementally adding the modules, checking the data flow between modules and ensuring that the module added is the child of the previous module.
-Bottom-up approach: Incrementally adding the modules, checking the data flow between modules and ensuring that the module added is the parent of the previous module.
-Sandwich approach: Combination of top-down and bottom-up approaches.
*Non-Incremental integration testing:
Adding all the modules in one single shot and checking the data flow between the modules. 
Drawbacks:
*We might miss some data flow between the modules.
*If there is a defect, we might not be able to find the root cause.

System testing: checking the functionality of the application with respect to client's requirements. It is a black-box testing technique and done by testers. After the completion of component and integration level testings, we start this. And before starting this, we need to know the client's requirements. It involves:
*User Interface testing(GUI): testing the interface of the application which involves elements like menus, checkboxes, buttons, fonts, sizes, colors, images, etc.
*Usability testing: testing to check how well the users are able to understand and operate the application.
*Functional testing: nothing but behavior of the application, it tells how your feature should work. It involves:
-Object-properties testing: enable, disable, visible, focus, etc.
-Database testing: column level validation(column length, column type, number of columns, etc.), relations between tables(normalization), functions, procedures, triggers, indexes, views, etc. It is testing the user operations with respect to database operations.
-Error handling: testing the error messages(should be readable) while providing incorrect inputs to the application.
-Calculations/manipulations: verifying the calculations with respect to requirements.
-Links existence and links execution: links existence is checking if the links are properly placed or not and links execution is checking if the links are navigating to the right page or not and this involves internal links(navigating to same page but different section), external links(navigating to different page) and broken links(no action/navigation).
-cookies and sessions: cookies are temporary files created by browser while browsing through the internet and sessions are time slots created by the browser, if idle for sometime without any action then the session expires.
*Non-functional testing: after the application functionality is stable then we do non-functional testing which involves performance, load, security, etc.
-Performance testing: checking the speed of the application like how well the app is responding. It involves:
1)Load testing: slowly/gradually increasing the load on the application and checking the speed.
2)Stress testing: suddenly increasing or decreasing the load on the application and checking the speed.
3)Volume testing: checking how much volume is being handled by the application.
-Security testing: checking how secure the app is. It involves:
1)Authentication: checking for valid users.
2)Authorization/Access control: setting permissions to valid users specifying what features they can use.
-Recovery testing: testing the application state change from abnormal to normal.
-Compatibility testing: testing for compatible features. Forward compatibility(updates), backward compatibility(other systems) and hardware compatibility(configuration testing).
-Installation testing: testing the screen navigation(simple or not) and also uninstallation process.
-Sanitation/garbage testing: if any application provides extra features then they are considered as bugs.

User Acceptance testing: done by users and testers will also be involved sometimes. It involves:
*Alpha testing: testing is done in the development environment.
*Beta testing: software is installed in customer environment and basic testing is done.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Software testing terminology:
Regression testing: conducted on modified modules to ensure that there is no impact on the existing functionalities with changes like adding/deleting/modifying builds. It involves:
*Unit regression testing: testing only the modified modules.
*Regional regression testing: testing the modified modules alomg with impacted modules. Impact analysis meeting is conducted with developer and QA to identify the impacted modules.
*Full regression testing: testing the main functionality and remaining parts of the application. If developer has done changes in many modules, then instead of testing separately we do one round of full regression.

Retesting: whenever developer fixes a bug, tester will work on that bug fix and if its proper then tester closes it or else he will reopen it and send it to the developer again. To ensure the defects which were found in the earlier build were fixed or not in the present build is retesting.

Smoke testing: It is done whenever developer releases a new build. Done by testers on initial builds to check if the build is stable/testable or not. 

Sanity testing: It is done during build release, where we test for main functionalities without going any deeper. Build is relatively stable and done on stable builds. Done whenever there is no time to do in-depth testing.

Exploratory testing: app should be explored, should understand all possible scenarios, document them and then use it for testing. This is done when the app is ready but there is no requirement. Tester should explore the app.
Drawbacks:
*We might mistake a feature for bug and a bug for feature.

Adhoc testing: testing the app randomly. This is done when the app is ready but there is no business requirement document. Tester should have the knowledge of the application. This is an invalid testing and can be done anytime.

Monkey/Gorilla testing: this is done when the app is ready but there is no business requirement document. Tester is not required to have the knowledge of the application. This is suitable for gaming apps.

Positive testing: testing the app with valid inputs and checking whether the app behaves as expected with positive inputs.
Example: set a text box which would only accept numbers(0 to 99999) and giving positive inputs(numbers) to check the behavior.

Negative testing: testing the app with invalid inputs and checking whether the app behaves as expected with negative inputs.
Example: set a text box which would only accept numbers(0 to 99999) and giving negative inputs(other than numbers) to check the behavior.

End-to-end testing: testing the overall functionalities of the system including the data integration of all modules.
Example: login-add new customer-edit customer-delete customer details-logout.

Globalisation/internalisation testing: testing whether the app runs globally. Different aspects like languages, currency formats, mobile number formats, address formats, etc are tested.
Example: Facebook is a globalised product and supports many languages and can be accessed by everyone in different countries.

Localisation testing: testing whether the app runs in specific/local regions. Only specific aspects like regional languages and other formats are tested.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Test design techniques: used to prepare test data. 
*Equivalence class partition(ECP): partitioning the data into classes and then test the data according to the class. Reduces the data(valid and invalid), more coverage and saves time.
*Boundary value analysis: used to test the boundaries of the inputs, min-1, min, min+1, max-1, max, max+1.
*Decision table: we use this when we have more input conditions and corresponding actions.
*State transition: this changes the state of the input. Allows the tester to test the behavior of the application. Tester can perform this action by entering a sequence of inputs. Testing team provides positive as well as negative inputs to evaluate the system behavior.
*Error guessing: used to find the bugs in the application based on tester's prior experience and analytical skills. Doesn't follow specific rules.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Software Testing Life Cycle(STLC): is a part of SDLC that is Testing phase. It involves:
*Requirement analysis.
*Test planning.
*Test design.
*Test execution.
*Defect/bug tracking and reporting.
*Test closure.

Test plan: is a document which describes the test scope, strategy, objectives, schedule, deliverables and resources required. Test plan template contents:
*Scope(inclusions, environments and exclusions).
*Test strategy.
*Defect reporting procedure.
*Roles/Responsibilities.
*Test schedule.
*Test deliverables.
*Pricing.
*Entry and exit criteria.
*Suspension and resumption criteria.
*Tools.
*Risks and mitigation.
*Approvals.

Use case: describes the requirement. Prepared by the Business Analyst(BA). It contains three items:
*Actor: which is the user, can be a single person or a group of people interacting with process.
*Action: which is to reach the final outcome.
*Goal/Outcome: which is the successful user outcome.

Test scenario: a possible area to be tested(What to test).

Test case: step by step actions to be performed to validate the functionality of the application(How to test). Test case contains test steps prepared by test engineer, expected result and actual result.

Test suite: a group of test cases which belong to the same category.

Test case contents:
*Test case ID.
*Test case title.
*Description.
*Preconditions.
*Priority.
*Requirement ID.
*Steps/Actions.
*Expected result.
*Actual result.
*Test data.

Requirement traceability matrix(RTM): describes the mapping of requirements with test cases. Its purpose is to check if all test cases are covered so that no functionality is missed while testing. It contains:
*Requirement ID.
*Requirement description.
*Test case ID.

Test execution: during this phase the testing team will carry out the testing based on test cases and test plan.
*Entry criteria: Test plan, test cases and test data.
*Activities:
-The test cases are executed based on test planning.
-The status of test cases is marked like passed, failed, blocked, run and others.
-Documentation of test results and log defects for failed cases is done.
-All blocked and failed cases are assigned bug IDs.
-Retesting once the defects are fixed.
-Defects are tracked till closure.  
-Deliverables: provides defect and test case execution report with completed results. 

Test environment: is a platform built specially for test case execution on software product. It is created by integrating required software and hardware along with proper network configurations. Test environment simulates production/real time environment. It is also called as test bed.

Defects/bugs: Any mismatched functionality in the application is a defect/bug/issue. Test engineers report these mismatches as defects to the development team through templates or tools. Some defect reporting tools are:
*ClearQuest.
*DevTrack.
*Jira.
*Quality Center.
*Bug Jilla.

Defect report contents:
*Defect ID.
*Defect Description.
*Version.
*Steps.
*Date raised.
*Reference.
*Detected By.
*Status.
*Fixed By.
*Date closed.
*Severity.
*Priority.

Defect classification:
Severity: describes the seriousness of the defect and the impact it has on business workflow. It has:
*Blocker:  show stopper, could not go any further like application crashed or login not working.
*Critical: main functionality of the application is not working. Example: could not order a product on an ecommerce platform.
*Major: it can cause some undesirable action but the application still works. Example: not receiving a confirmation mail/text after booking a cab.
*Minor: it does not have much impact. Look or feel issues. Example: spellings, alignments, etc.

Priority: describes the importance of the defect. It has:
P0: High. Needs to be fixed immediately as it effects the system severely and cannot be used until it is fixed.
P1: Medium. Developer can fix this until a new version of build is created.
P2: Low. Developer can fix it in later releases.

Based on these we get severity and priority combinations:
*High priority and high severity.
*High priority and low severity.
*Low priority and high severity.
*Low priority and low severity.

Defect resolution: After receiving the defect report from the testing team, the developer team conducts a review meeting to fix these defects. Then they will send a resolution type to the testing team for further communication. Resolution types: accept, reject, duplicate, enhancement, needs more information, not reproducible, fixed, as designed.

Test closure: activities involve:
*Evaluate the cycle completion criteria based on time, test coverage, cost, software, critical business objectives, quality.
*Prepare test metrics based on above parameters.
*Document the learning out of project.
*Prepare test summary report.
*Qualitative and quantitative reporting of quality of work product to the customer.
*Test result analysis to find out defect distribution by type and severity.
Deliverables:
*Test closure report.
*Test metrics.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

In general software testing involves:
*Project introduction.
*Understanding and exploring the functionality.
*Test plan.
*Writing the test scenarios.
*Writing test cases and reviews.
*Environment setup and build deployment.
*Test execution.
*Bug tracking and reporting.
*Sanity testing, retesting and regression testing.
*Test sign off.

Seven principles of software testing:
*Start the testing at the early stages. Means from the beginning when you get the requirements.
*Genuinely test the software in order to find defects.
*It is highly impossible to give bug-free software to the customer.
*Should not do exhaustive testing. Means you should not use the same type of data for testing every time.
*Testing is context based. Means based on the type of application, you have to decide what types of testing should be conducted.
*The process should be followed as pesticide paradox. Means you cannot execute the same test cases for longer run, you need to update the test cases for every cycle/release in order to find more defects.
*The process should be followed as defect clustering. Means some modules contain most of the defects and with experience we can identify such risky modules. 80% of defects are found in 20% of modules.

QA activities in testing:
*Understanding the requirements and functional specifications of the application.
*Identifying required test scenarios.
*Designing test cases to validate the app.
*Setting up test environment(test bed).
*Executing test cases to validate the app.
*Logging the test results(how many cases passed/failed).
*Defect reporting and tracking.
*Retest the fixed defects from previous build.
*Perform various types of testing on the application.
*Report to team lead with the status of assigned tasks.
*Participate in regular team meetings.
*Creating automation scripts.
*Providing suggestions/recommendations/opinions about whether or not the application is ready for production.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Agile model/Agile methodology/Agile process: An iterative or incremental process.
*Customer doesn't need to wait for longer period of time.
*We develop, test and release pieces of software with few number of features.
*We can accept or accommodate requirement changes.
There will be good communication between customer, business analyst, developers and testers.

Advantages:
*Requirement changes are allowed in any stage of development.
*Releases will be very fast(weekly).
*Easy model to adopt.

Disadvantages:
Less focus on design and documentation since we deliver software faster.

Scrum: A framework through which we build the software product by following agile principles. Includes a group of people called scrum team(5 to 9 people).
-Product owner:
*Defines the features of the product.
*Prioritizes the features according to market value.
*Adjusts features and priority after every iteration as needed.
*Accepts/rejects the work results.

-Scrum master: Main role in facilitating and driving the agile process.

-Developers and testers: Develop and test the software.

Scrum terminology:
*User story: A feature/module in software.
*Epic: A collection of user stories.
*Product backlog: Contains a list of user stories prepared by product owner.
*Sprint/Iteration: Period of time to complete the user stories decided by product owner and team, usually 2 to 4 weeks of time.
*Sprint planning meeting: Meeting conducted with team to define what can be delivered in the sprint duration.
*Sprint backlog: List of committed user stories by developer/QA for specific sprint.
*Scrum meeting: Meeting conducted by scrum master everyday for 15 minutes. It includes discussions like:
-What did you do yesterday.
-What will you do today.
-Are there any impediments in your way.
*Sprint retrospective meeting: Meeting conducted after the completion of sprint. The entire team including scrum master and product owner should participate. Discussions about what went well, what improvements can be made for upcoming sprints, etc. are done.
*Story point: A rough estimation of user stories given by developer and QA in the form of Fibonacci series(0, 1, 1, 2, 3, 5, 8,....).
*Burndown chart: Shows how much work is remaining in the sprint and is maintained by the scrum master daily.

Roles: product owner, scrum master and team.
Artifacts: product backlog, sprint backlog and burndown chart.
Ceremonies: sprint planning meeting, daily scrum and sprint review.

Scrum board: contains Stories, To-do, In-progress, Testing, Done, etc.

Definition of Ready(DOR):
*User story is clear, it is feasible, it is testable, acceptance criteria is defined, dependencies identified, sized by developer team.
*Scrum team accepts the user experience artifacts.
*Performance criteria is identified where appropriate.
*Team has a good idea as to what it will mean to demo the user story.

Definition of Done(DOD):
*Code produced(all to-do items in code completed).
*Code commented, checked-in and run against current version in source control.
*Peer reviewed(or produced with pair programming) and meeting development standards.
*Builds without errors.
*Unit tests written and passing.
*Deployed to system test environment and system tests passed.
*Deployed to UAT test environment and UAT tests passed and signed off as meeting requirements.
*Any changes in build/deployment/configuration are implemented/documented/communicated.
*Relevant documentation/diagrams produced or updated.
*Remaining hours of task set to zero and task closed.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

















































